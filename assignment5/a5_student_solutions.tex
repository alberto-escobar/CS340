% !TEX enableShellEscape = yes
% (The above line makes atom's latex package compile with -shell-escape
% for minted, and is just ignored by other systems.)
\documentclass{article}

\usepackage{fullpage}
\usepackage{color}
\usepackage{amsmath,amssymb}
\usepackage{url}
\usepackage{verbatim}
\usepackage{graphicx}
\usepackage{parskip}
\usepackage{amssymb}
\usepackage{hyperref}

% Use one or the other of these for displaying code.
% NOTE: If you get
%  ! Package minted Error: You must invoke LaTeX with the -shell-escape flag.
% and don't want to use minted, just comment out the next line
\usepackage{minted} \BeforeBeginEnvironment{minted}{\begingroup\color{black}} \AfterEndEnvironment{minted}{\endgroup} \setminted{autogobble,breaklines,breakanywhere,linenos}

\usepackage{listings}

% Colours
\definecolor{blu}{rgb}{0,0,1}
\newcommand{\blu}[1]{{\textcolor{blu}{#1}}}
\definecolor{gre}{rgb}{0,.5,0}
\newcommand{\gre}[1]{\textcolor{gre}{#1}}
\definecolor{red}{rgb}{1,0,0}
\newcommand{\red}[1]{\textcolor{red}{#1}}
\definecolor{pointscolour}{rgb}{0.6,0.3,0}

% answer commands
\newcommand\ans[1]{\par\gre{Answer: #1}}
\newenvironment{answer}{\par\begingroup\color{gre}Answer: }{\endgroup}
\let\ask\blu
\let\update\red
\newenvironment{asking}{\begingroup\color{blu}}{\endgroup}
\newcommand\pts[1]{\textcolor{pointscolour}{[#1~points]}}

% Math
\def\R{\mathbb{R}}
\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\mat}[1]{\begin{bmatrix}#1\end{bmatrix}}

% LaTeX
\newcommand{\fig}[2]{\includegraphics[width=#1\textwidth]{#2}}
\newcommand{\centerfig}[2]{\begin{center}\includegraphics[width=#1\textwidth]{#2}\end{center}}

\begin{document}

\title{CPSC 340 Assignment 5}
\author{}
\date{}
\maketitle
\vspace{-6em}

\section*{Important: Submission Format \pts{5}}

Please make sure to follow the submission instructions posted on the course website.
\ask{We will deduct marks if the submission format is incorrect, or if you're not using \LaTeX{} and your submission is \emph{at all} difficult to read} -- at least these 5 points, more for egregious issues.
Compared to assignment 1, your name and student number are no longer necessary (though it's not a bad idea to include them just in case, especially if you're doing the assignment with a partner).

\vspace{1em}



\section{Kernel Logistic Regresion \pts{22}}

If you run \verb|python main.py 1| it will load a synthetic 2D data set, split it into train/validation sets, and then perform regular logistic regression and kernel logistic regression (both without an intercept term, for simplicity). You'll observe that the error values and plots generated look the same, since the kernel being used is the linear kernel (i.e., the kernel corresponding to no change of basis). Here's one of the two identical plots:
\centerfig{0.5}{figs/logReg.png}


\subsection{Implementing kernels \pts{8}}

Inside \texttt{kernels.py}, you will see classes named \texttt{PolynomialKernel} and \texttt{GaussianRBFKernel}, whose \verb|__call__| methods are yet to be written.
\begin{asking}
  Implement the polynomial kernel and the RBF kernel for logistic regression.
  Report your training/validation errors and submit the plots from \verb|utils.plot_classifier| for each case.
\end{asking}
You should use the kernel hyperparameters $p=2$ and $\sigma=0.5$ respectively,
and $\lambda=0.01$ for the regularization strength.
For the Gaussian kernel, please do \emph{not} use a $1/\sqrt{2\pi\sigma^2}$ multiplier.
\newpage

\begin{ans}{\newline
Polynomial Kernel \newline
Training Error: 18.3\% \newline
Validation Error: 17.0\%
\newline \newline
Gaussian RBF Kernel \newline
Training Error: 12.7\% \newline
Validation Error: 11.0\%}
\end{ans}

\begin{minted}{python}
class PolynomialKernel(Kernel):
    def __init__(self, p):
        """
        p is the degree of the polynomial
        """
        self.p = p

    def __call__(self, X1, X2):
        """
        Evaluate the polynomial kernel.
        A naive implementation will use change of basis.
        A "kernel trick" implementation bypasses change of basis.
        """

        """YOUR CODE HERE FOR Q1.1"""
        return (X1 @ X2.T + 1) ** self.p
\end{minted}
\centerfig{0.5}{figs/logRegPolynomial.png}

\newpage
\begin{minted}{python}
class GaussianRBFKernel(Kernel):
    def __init__(self, sigma):
        """
        sigma is the curve width hyperparameter.
        """
        self.sigma = sigma

    def __call__(self, X1, X2):
        """
        Evaluate Gaussian RBF basis kernel.
        """

        """YOUR CODE HERE FOR Q1.1"""
        X_distance = euclidean_dist_squared(X1, X2)
        return np.exp(-X_distance/(2*self.sigma**2))
\end{minted}
\centerfig{0.5}{figs/logRegGaussianRBF.png}

\newpage
\subsection{Hyperparameter search \pts{10}}

For the RBF kernel logistic regression, consider the hyperparameter values $\sigma=10^m$ for $m=-2,-1,\ldots,2$ and $\lambda=10^m$ for $m=-4,-3,\ldots,2$.
The function \verb|q1_2()| has a little bit in it already to help set up to run a grid search over the possible combination of these parameter values.
You'll need to fill in the \verb|train_errs| and \verb|val_errs| arrays
with the results on the given training and validation sets, respectively;
then the code already in the function will produce a plot of the error grids.
\ask{Submit this plot}.
Also, for each of the training and testing errors,
pick the best (or one of the best, if there's a tie) hyperparameters for that error metric,
and \ask{report the parameter values and the corresponding error, as well as a plot of the decision boundaries (plotting only the training set)}.
While you're at it, \ask{submit your code}.
To recap, for this question you should be submitting: gridsearch plot,
two decision boundary plots,
the values of two hyperparameter pairs with corresponding errors,
and your code. 


Note: on the real job you might choose to use a tool like scikit-learn's \texttt{GridSearchCV} to implement the grid search, but here we are asking you to implement it yourself, by looping over the hyperparameter values.

\ans{}
\centerfig{1.0}{figs/logRegRBF_grids.png}
\gre{Best Training Error Decision Boundary \newline The best training error is 0.0\% which occurs with sigma = 0.01 and lambda = 0.0001, and has a validation error of 21.0\%}
\newline
\centerfig{0.5}{figs/logRegGaussianRBFBestTrainError.png}
\gre{Best Validation Error Decision Boundary \newline The best validation error is 12.0\% which occurs with sigma = 0.1 and lambda = 1.0, and has a training error 9.3\%}
\centerfig{0.5}{figs/logRegGaussianRBFBestValError.png}

\begin{minted}{python}
def q1_2():
    X_train, y_train, X_val, y_val = load_and_split("nonLinearData.pkl")
    optimizer = GradientDescentLineSearch()

    sigmas = 10.0 ** np.array([-2, -1, 0, 1, 2])
    lammys = 10.0 ** np.array([-4, -3, -2, -1, 0, 1, 2])

    # train_errs[i, j] should be the train error for sigmas[i], lammys[j]
    train_errs = np.full((len(sigmas), len(lammys)), 100.0)
    val_errs = np.full((len(sigmas), len(lammys)), 100.0)  # same for val
    min_train_err = 1
    min_train_err_i = -1
    min_train_err_j = -1

    min_val_err = 1
    min_val_err_i = -1
    min_val_err_j = -1

    for i in range(len(sigmas)):
        for j in range(len(lammys)):
            loss_fn = KernelLogisticRegressionLossL2(lammys[j])
            kernel = GaussianRBFKernel(sigmas[i])
            klr_model = KernelClassifier(loss_fn, optimizer, kernel)
            klr_model.fit(X_train, y_train)

            train_errs[i][j] = np.mean(klr_model.predict(X_train) != y_train)
            if train_errs[i][j] < min_train_err:
                min_train_err = train_errs[i][j]
                min_train_err_i = i
                min_train_err_j = j

            val_errs[i][j] = np.mean(klr_model.predict(X_val) != y_val)
            if val_errs[i][j] < min_val_err:
                min_val_err = val_errs[i][j]
                min_val_err_i = i
                min_val_err_j = j

    print(f"Min Train error: {min_train_err:.2f} at sigma value: {sigmas[min_train_err_i]} and lambda value: {lammys[min_train_err_j]}")
    print(f"Min Val error: {min_val_err:.2f} at sigma value: {sigmas[min_val_err_i]} and lambda value: {lammys[min_val_err_j]}")

    # save plot of minimum training error
    loss_fn = KernelLogisticRegressionLossL2(lammys[min_train_err_j])
    kernel = GaussianRBFKernel(sigmas[min_train_err_i])
    klr_model = KernelClassifier(loss_fn, optimizer, kernel)
    klr_model.fit(X_train, y_train)
    print(f"Training error {np.mean(klr_model.predict(X_train) != y_train):.1%}")
    print(f"Validation error {np.mean(klr_model.predict(X_val) != y_val):.1%}")
    fig = plot_classifier(klr_model, X_train, y_train)
    savefig("logRegGaussRBFMinTrainError.png", fig)

    # save plot of minimum validation error
    loss_fn = KernelLogisticRegressionLossL2(lammys[min_val_err_j])
    kernel = GaussianRBFKernel(sigmas[min_val_err_i])
    klr_model = KernelClassifier(loss_fn, optimizer, kernel)
    klr_model.fit(X_train, y_train)
    print(f"Training error {np.mean(klr_model.predict(X_train) != y_train):.1%}")
    print(f"Validation error {np.mean(klr_model.predict(X_val) != y_val):.1%}")
    fig = plot_classifier(klr_model, X_train, y_train)
    savefig("logRegGaussRBFMinValidationError.png", fig)

    # Make a picture with the two error arrays. No need to worry about details here.
\end{minted}

\subsection{Reflection \pts{4}}
\ask{
Briefly discuss the best hyperparameters you found in the previous part, and their associated plots. Was the training error minimized by the values you expected, given the ways that $\sigma$ and $\lambda$ affect the fundamental tradeoff?
}
\ans{With a small $\sigma$ and $\lambda$, the model is highly complex and minimally regularized. This configuration allows the model to effectively memorize the training data, leading to a training error of nearly zero. However, this comes at the cost of over fitting, as the model does not generalize well to unseen data. \newline\newline On the other hand, when $\sigma = 0.01$ and $\lambda = 1$, the model becomes simpler and incorporates more regularization. This strikes a better balance between bias and variance, resulting in improved performance on the validation set. The increased $\lambda$ reduces the risk of over fitting by penalizing large weights, while the small $\sigma$ ensures the Gaussian RBF kernel captures local structure effectively but avoids excessive complexity.\newline\newline This behavior aligns with the fundamental tradeoff: smaller $\sigma$ increases model capacity, allowing finer detail to be captured, while larger $\lambda$ enforces smoother and more generalized decision boundaries. The optimal hyper parameters reflect a compromise that minimizes validation error while avoiding over fitting.}
\clearpage
\section{MAP Estimation \pts{16}}

In class, we considered MAP estimation in a regression model where we assumed that:
\begin{itemize}
\item The likelihood $p(y_i \mid x_i, w)$ comes from a normal density with a mean of $w^Tx_i$ and a variance of $1$.
\item The prior for each variable $j$, $p(w_j)$, is a normal distribution with a mean of zero and a variance of $\lambda^{-1}$.
\end{itemize}
Under these assumptions, we showed that this leads to the standard L2-regularized least squares objective function,
\[
f(w) = \frac{1}{2}\norm{Xw - y}^2 + \frac \lambda 2 \norm{w}^2,
\]
which is the negative log likelihood (NLL) under these assumptions (ignoring an irrelevant constant).
\ask{For each of the alternate assumptions below, show the corresponding loss function} \pts{each 4}. Simplify your answer as much as possible, including possibly dropping additive constants.
\begin{enumerate}

\item We use a Gaussian likelihood where each datapoint has its own variance $\sigma_i^2$, and a zero-mean Laplace prior with a variance of $\lambda^{-1}$.
\[
p(y_i \mid x_i,w) = \frac{1}{\sqrt{2\sigma_i^2\pi}}\exp\left(-\frac{(w^Tx_i - y_i)^2}{2\sigma_i^2}\right), \quad p(w_j) = \frac{\lambda}{2}\exp(-\lambda|w_j|).
\]

You can use $\Sigma$ as a diagonal matrix that has the values $\sigma_i^2$ along the diagonal.
\begin{answer}
    \[
    w = \mathop{\mathrm{arg\,max}}_w p(w_j)\Pi^n_{i=1}p(y_i \mid x_i,w)
    \]
    \[
    = \mathop{\mathrm{arg\,min}}_w -log(p(w_j))-\Sigma^n_{i=1}log(p(y_i \mid x_i,w))
    \]
    \[
    f(w) = -log(\frac{\lambda}{2}\exp(-\lambda|w_j|))-\Sigma^n_{i=1}log(\frac{1}{\sqrt{2\sigma_i^2\pi}}\exp\left(-\frac{(w^Tx_i - y_i)^2}{2\sigma_i^2}\right)) 
    \]
    \[
    = -log(\frac{\lambda}{2})-log(\exp(-\lambda|w_j|))-\Sigma^n_{i=1}log(\frac{1}{\sqrt{2\sigma_i^2\pi}})+log(\exp\left(-\frac{(w^Tx_i - y_i)^2}{2\sigma_i^2}\right))
    \]
    \[
    = -log(\exp(-\lambda|w_j|))-\Sigma^n_{i=1}log(\exp\left(-\frac{(w^Tx_i - y_i)^2}{2\sigma_i^2}\right))
    \]
    \[
    = \lambda|w_j|+\Sigma^n_{i=1}\frac{(w^Tx_i - y_i)^2}{2\sigma_i^2}
    \]
    \[
    = \lambda||w||_1+\frac{1}{2}||\Sigma^{-1/2}(Xw-y)||^2_2
    \]
\end{answer}

\newpage

\item We use a Laplace likelihood with a mean of $w^Tx_i$ and a variance of $8$, and we use a zero-mean Gaussian prior with a variance of $\sigma^2$:
\[
p(y_i \mid x_i, w) = \frac14 \exp\left(- \frac12 |w^Tx_i - y_i| \right), \quad
p(w_j) = \frac{1}{\sqrt{2\pi} \, \sigma} \exp\left(-\frac{w_j^2}{2\sigma^2} \right).
\]

\begin{answer}
    \[
    w = \mathop{\mathrm{arg\,max}}_w p(w_j)\Pi^n_{i=1}p(y_i \mid x_i,w)
    \]
    \[
    = \mathop{\mathrm{arg\,min}}_w -log(p(w_j))-\Sigma^n_{i=1}log(p(y_i \mid x_i,w))
    \]
    \[
    f(w) = -log(\frac{1}{\sqrt{2\pi} \, \sigma} \exp\left(-\frac{w_j^2}{2\sigma^2} \right))-\Sigma^n_{i=1}log(\frac14 \exp\left(- \frac12 |w^Tx_i - y_i| \right)) 
    \]
    \[
    = -log(\frac{1}{\sqrt{2\pi} \, \sigma})-log( \exp\left(-\frac{w_j^2}{2\sigma^2} \right))-\Sigma^n_{i=1}log(\frac14) +log( \exp\left(- \frac12 |w^Tx_i - y_i| \right)) 
    \]
    \[
    = -log( \exp\left(-\frac{w_j^2}{2\sigma^2} \right))-\Sigma^n_{i=1}log( \exp\left(- \frac12 |w^Tx_i - y_i| \right)) 
    \]
    \[
    = \frac{w_j^2}{2\sigma^2} +\Sigma^n_{i=1} \frac12 |w^Tx_i - y_i|
    \]
    \[
    = \frac{1}{2\sigma^2}||w||^2_2+\frac12||Xw - y||_1 
    \]
\end{answer}
\newpage
 \item We use a (very robust) student $t$ likelihood with a mean of $w^Tx_i$ and $\nu$ degrees of freedom, and a Gaussian prior with a mean of $\mu_j$ and a variance of $\lambda^{-1}$,
\[
  p(y_i \mid x_i, w) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\frac \nu 2\right)}
                       \left(1 + \frac{(w^T x_i - y_i)^2}{\nu} \right)^{-\frac{\nu+1}{2}}
, \quad
  p(w_j) = \sqrt{\frac{\lambda}{2\pi}} \exp\left( -\frac\lambda2 (w_j - \mu_j)^2 \right).
\]
where $\Gamma$ is the \href{https://en.wikipedia.org/wiki/Gamma_function}{gamma function} (which is always non-negative).
You can use $\mu$ as a vector whose components are $\mu_j$.
\begin{answer}
    \[
    w = \mathop{\mathrm{arg\,max}}_w p(w_j)\Pi^n_{i=1}p(y_i \mid x_i,w)
    \]
    \[
    = \mathop{\mathrm{arg\,min}}_w -log(p(w_j))-\Sigma^n_{i=1}log(p(y_i \mid x_i,w))
    \]
    \[
    f(w) = -log(\sqrt{\frac{\lambda}{2\pi}} \exp\left( -\frac\lambda2 (w_j - \mu_j)^2 \right))-\Sigma^n_{i=1}log(\frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\frac \nu 2\right)}\left(1 + \frac{(w^T x_i - y_i)^2}{\nu} \right)^{-\frac{\nu+1}{2}}) 
    \]
    \[
    = -log(\sqrt{\frac{\lambda}{2\pi}})-log( \exp\left( -\frac\lambda2 (w_j - \mu_j)^2 \right))-\Sigma^n_{i=1}(log(\frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\frac \nu 2\right)})+log\left(1 + \frac{(w^T x_i - y_i)^2}{\nu} \right)^{-\frac{\nu+1}{2}}) 
    \]
    \[
    = \frac\lambda2 (w_j - \mu_j)^2 +\Sigma^n_{i=1}\frac{\nu+1}{2}log\left(1 + \frac{(w^T x_i - y_i)^2}{\nu} \right) 
    \]
    \[
    = \frac\lambda2 ||w - \mu||_2^2 +\frac{\nu+1}{2}\Sigma^n_{i=1}log\left(1 + \frac{(w^T x_i - y_i)^2}{\nu} \right) 
    \]
\end{answer}
\newpage
\item We use a Poisson-distributed likelihood (for the case where $y_i$ represents counts), and a uniform prior for some constant $\kappa$,
\[
p(y_i | w^Tx_i) = \frac{\exp(y_iw^Tx_i)\exp(-\exp(w^Tx_i))}{y_i!}, \quad p(w_j) \propto \kappa.
\]
(This prior is 	``improper'', since $w\in\R^d$ but $\kappa$ doesn't integrate to 1 over this domain. Nevertheless, the posterior will be a proper distribution.)
\begin{answer}
    \[
    w = \mathop{\mathrm{arg\,max}}_w p(w_j)\Pi^n_{i=1}p(y_i \mid x_i,w)
    \]
    \[
    = \mathop{\mathrm{arg\,min}}_w -log(p(w_j))-\Sigma^n_{i=1}log(p(y_i \mid x_i,w))
    \]
    \[
    f(w) = -log(\kappa)-\Sigma^n_{i=1}log(\frac{\exp(y_iw^Tx_i)\exp(-\exp(w^Tx_i))}{y_i!}) 
    \]
    \[
    = \Sigma^n_{i=1}\exp(w^Tx_i)-(y_iw^Tx_i)
    \]
\end{answer}
\end{enumerate}


\clearpage
\section{Principal Component Analysis \pts{19}}
\subsection{PCA by Hand \pts{6}}

Consider the following dataset, containing 5 examples with 3 features each:
\begin{center}
  \begin{tabular}{ccc}
    $x_1$ & $x_2$ & $x_3$ \\
    \hline
     0 &  2 &  0 \\
     3 & -4 &  3 \\
     1 &  0 &  1 \\
    -1 &  4 & -1 \\
     2 & -2 &  2 \\
  \end{tabular}
\end{center}
Recall that with PCA we usually assume we centre the data before applying PCA (so it has mean zero).
We're also going to use the usual form of PCA where the PCs are normalized ($\norm{w} = 1$),
and the direction of the first PC is the one that minimizes the orthogonal distance to all data points.
\update{We're only going to consider $k = 1$ component here.}
\begin{enumerate}
  \item \ask{What is the first principal component?}

\begin{answer}
First, we compute the mean of each feature:

$u_1 = (0+3+1-1+2)/5 = 1$ \newline
$u_2 = (2-4+0+4-2)/5 = 0$ \newline
$u_3 = (0+3+1-1+2)/5 = 1$ \newline

Then, subtract each value in the table by the corresponding mean value, to obtain the centered data:

\begin{center}
  \begin{tabular}{ccc}
    $x_1$ & $x_2$ & $x_3$ \\
    \hline
     -1 &  2 &  -1 \\
     2 & -4 &  2 \\
     0 &  0 &  0 \\
    -2 &  4 & -2 \\
     1 & -2 &  1 \\
  \end{tabular}
\end{center}

By observing the data and the plot, one can conclude that the first principal component before normalization is
\[
\begin{bmatrix}
1 & -2 & 1
\end{bmatrix}.
\]

After normalizing using the denominator \(\sqrt{6}\), we obtain the first principal component:
\[ w_1 =
\begin{bmatrix}
\frac{1}{\sqrt{6}} & \frac{-2}{\sqrt{6}} & \frac{1}{\sqrt{6}}
\end{bmatrix}.
\]


\end{answer}

\newpage
  \item \ask{What is the reconstruction loss (L2 norm squared) of the point $(2.5, -3, 2.5)$? (Show your work.)}

\begin{answer}
\[
\text{First, center the new data:} \quad \tilde{X} = X - \mu =
\begin{bmatrix}
2.5 - 1 \\
-3 - 0 \\
2.5 - 1
\end{bmatrix}^T
=
\begin{bmatrix}
1.5 \\
-3 \\
1.5
\end{bmatrix}^T.
\]

Next, find \(\tilde{Z} = \tilde{X} W^T (W W^T)^{-1}\):
\[
\tilde{Z} =
\begin{bmatrix}
1.5 \\
-3 \\
1.5
\end{bmatrix}^T
\begin{bmatrix}
\frac{1}{\sqrt{6}} \\
-\frac{2}{\sqrt{6}} \\
\frac{1}{\sqrt{6}}
\end{bmatrix}
\begin{bmatrix}
\frac{6}{6}
\end{bmatrix}^{-1}
=
\frac{1.5 + 6 + 1.5}{\sqrt{6}}
=
\frac{9}{\sqrt{6}}.
\]

Then, plug it into reconstruction loss:
\[
\| \tilde{Z} W - \tilde{X} \|_F^2 = \left\|
\frac{9}{\sqrt{6}\sqrt{6}}
\begin{bmatrix}
1 & -2 & 1
\end{bmatrix}
-
\begin{bmatrix}
1.5 & -3 & 1.5
\end{bmatrix}
\right\|_F^2
\]

\[
= \left\|
\begin{bmatrix}
\frac{9}{6} - 1.5 \\
-\frac{18}{6} + 3 \\
\frac{9}{6} - 1.5
\end{bmatrix}
\right\|_F^2
= 2 
\left(\frac{9}{6} - 1.5\right)^2 + \left(-\frac{18}{6} + 3\right)^2) = 0.
\]


\end{answer}
  
  \item \ask{What is the reconstruction loss (L2 norm squared) of the point $(1, -3, 2)$? (Show your work.)}

\begin{answer}
\[
\text{First, center the new data:} \quad \tilde{X} = X - \mu =
\begin{bmatrix}
1 - 1 \\
-3 - 0 \\
2 - 1
\end{bmatrix}^T
=
\begin{bmatrix}
0 \\
-3 \\
1
\end{bmatrix}^T.
\]

Next, find \(\tilde{Z} = \tilde{X} W (W W^T)^{-1}\):
\[
\tilde{Z} =
\begin{bmatrix}
0 \\
-3 \\
1
\end{bmatrix}^T
\begin{bmatrix}
\frac{1}{\sqrt{6}} \\
-\frac{2}{\sqrt{6}} \\
\frac{1}{\sqrt{6}}
\end{bmatrix}
\begin{bmatrix}
\frac{6}{6}
\end{bmatrix}^{-1}
=
\frac{6 + 1}{\sqrt{6}}
=
\frac{7}{\sqrt{6}}.
\]

Then, plug it into reconstruction loss:
\[
\| \tilde{Z} W - \tilde{X} \|_F^2 = \left\|
\frac{7}{\sqrt{6}\sqrt{6}}
\begin{bmatrix}
1 & -2 & 1
\end{bmatrix}
-
\begin{bmatrix}
0 & -3 & 1
\end{bmatrix}
\right\|_F^2
\]

\[
= \left\|
\begin{bmatrix}
\frac{7}{6} \\
-\frac{14}{6} + 3 \\
\frac{7}{6} - 1
\end{bmatrix}
\right\|_F^2
= \left(\frac{7}{6}\right)^2 + \left(-\frac{14}{6} + 3\right)^2 + \left(\frac{7}{6} - 1\right)^2
= \frac{11}{6}.
\]

\end{answer}

\end{enumerate}
Hint: it may help (a lot) to plot the data before you start this question.



\subsection{Data Visualization \pts{7}}

If you run \verb|python main.py 3.2|, the program will load a dataset containing 50 examples, each representing an animal.
The 85 features are traits of these animals.
The script standardizes these features and gives two unsatisfying visualizations of it.
First, it shows a plot of the matrix entries, which has too much information and thus gives little insight into the relationships between the animals.
Next it shows a scatterplot based on two random features and displays the name of 15 randomly-chosen animals.
Because of the binary features even a scatterplot matrix shows us almost nothing about the data.

In \update{\texttt{encoders.py}}, you will find \update{a class named \texttt{PCAEncoder}}, which implements the classic PCA method (orthogonal bases via SVD) for a given $k$, the number of principal components. Using this class, create a scatterplot that uses the latent features $z_i$ from the PCA model with $k=2$.
Make a scatterplot of all examples using the first column of $Z$ as the $x$-axis and the second column of $Z$ as the $y$-axis, and use \texttt{plt.annotate()} to label the points corresponding to \verb|random_is| in the scatterplot.
(It's okay if some of the text overlaps each other; a fancier visualization would try to avoid this, of course, but hopefully you can still see most of the animals.)
Do the following:
\newpage
\begin{asking}
	\begin{enumerate}
		\item  Hand in your modified demo and the scatterplot.
        \ans{}
\begin{minted}{python}
    model = PCAEncoder(2)
    model.fit(X_train)
    W = model.W
    Z = X_train @ W.T @ np.linalg.inv(W @ W.T)
    print(Z)
    fig, ax = plt.subplots()
    ax.scatter(Z[:, 0], Z[:, 1])
    for i in random_is:
        ax.annotate(animal_names[i], xy=Z[i, :])
    savefig("animals_answer.png", fig)
    plt.close(fig)
    trait1 = trait_names[np.argmax(np.abs(W[0,:]))]
    trait2 = trait_names[np.argmax(np.abs(W[1,:]))]
    print(trait1, trait2)
\end{minted}
\centerfig{0.5}{figs/animals_answer.png}

		\item Which trait of the animals has the largest influence (absolute value) on the first principal component?
        \ans{paws}
		\item Which trait of the animals has the largest influence (absolute value) on the second principal component?
        \ans{vegetation}
    \end{enumerate}
\end{asking}
\newpage
\subsection{Data Compression \pts{6}}
It is important to know how much of the information in our dataset is captured by the low-dimensional PCA representation.
In class we discussed the ``analysis'' view that PCA maximizes the variance that is explained by the PCs, and the connection between the Frobenius norm and the variance of a centred data matrix $X$.
Use this connection to answer the following:
\begin{enumerate}
	\item \ask{How much of the variance is explained by our two-dimensional representation from the previous question?}
    \ans{Variance explained: 0.221}
    \begin{minted}{python}
     model = PCAEncoder(2)
    model.fit(X_train)
    W = model.W
    Z = X_train @ W.T @ np.linalg.inv(W @ W.T)
    X_centered = X_train - model.mu
    variance_explained = 1 - np.linalg.norm((Z@W - X_centered), ord='fro')**2 / np.linalg.norm((X_centered), ord='fro')**2
    print("Variance explained: {:.3f}".format(variance_explained))
    \end{minted}
	\item \ask{How many PCs are required to explain 50\% of the variance in the data?}
        \ans{At k = 7 the variance explained = 0.500}
        \begin{minted}{python}
    for k in range(1,100):
        model = PCAEncoder(k)
        model.fit(X_train)
        W = model.W
        Z = X_train @ W.T @ np.linalg.inv(W @ W.T)
        X_centered = X_train - model.mu
        variance_explained = 1 - np.linalg.norm((Z@W - X_centered), ord='fro')**2 / np.linalg.norm((X_centered), ord='fro')**2
        if variance_explained > 0.5:
            print("at k = {:d} the variance explained = {:.3f}".format(k, variance_explained))
            break
        \end{minted}
\end{enumerate}
Note: you can compute the Frobenius norm of a matrix using the function \texttt{np.linalg.norm}, among other ways. Also, note that the ``variance explained'' formula from class assumes that $X$ is already centred.


\clearpage
\section{Stochastic Gradient Descent \pts{20}}

If you run \verb|python main.py 4|, the program will do the following:
\begin{enumerate}
	\item Load the dynamics learning dataset ($n = 10000, d = 5$)
	\item Standardize the features
	\item Perform gradient descent with line search to optimize an ordinary least squares linear regression model
	\item Report the training error using \texttt{np.mean()}
	\item Produce a learning curve obtained from training
\end{enumerate}

The learning curve obtained from our \texttt{GradientDescentLineSearch} looks like this:
\centerfig{.6}{./figs/gd_line_search_curve.png}

This dataset was generated from a 2D bouncing ball simulation, where the ball is initialized with some random position and random velocity. The ball is released in a box and collides with the sides of the box, while being pulled down by the Earth's gravity. The features of $X$ are the position and the velocity of the ball at some timestep and some irrelevant noise. The label $y$ is the $y$-position of the ball at the next timestep. Your task is to train an ordinary least squares model on this data using stochastic gradient descent instead of the deterministic gradient descent.

\subsection{Batch Size of SGD \pts{5}}

In \texttt{optimizers.py}, you will find \texttt{StochasticGradient}, a \textit{wrapper} class that encapsulates another optimizer--let's call this a base optimizer.
\texttt{StochasticGradient} uses the base optimizer's \texttt{step()} method for each mini-batch to navigate the parameter space.
The constructor for \texttt{StochasticGradient} has two arguments: \texttt{batch\_size} and \texttt{learning\_rate\_getter}. The argument \texttt{learning\_rate\_getter} is an object of class \texttt{LearningRateGetter} which returns the ``current'' value learning rate based on the number of batch-wise gradient descent iterations. Currently. \texttt{ConstantLR} is the only class fully implemented.

\ask{Submit your code} from \texttt{main.py} that instantiates a linear model optimized with \texttt{StochasticGradient} taking \texttt{GradientDescent} (not line search!) as a base optimizer. Do the following:
\begin{enumerate}
		\item Use ordinary least squares objective function (no regularization).
		\item Using \texttt{ConstantLR}, set the step size to $\alpha^t = 0.0003$.
		\item Try the batch size values of $\texttt{batch\_size} \in \{1, 10, 100\}$.
\end{enumerate}
\ask{For each batch size value, use the provided training and validation sets to compute and report training and validation errors after 10 epochs of training. Compare these errors to the error obtained previously.}

\begin{answer}

Batch size: 1   Training error: 0.140   Validation error: 0.140

Batch size: 10  Training error: 0.140   Validation error: 0.140

Batch size: 100 Training error: 0.178   Validation error: 0.177

Previously we obtained a training and validation error of 0.14. For batch size of 1 and 10, the same training error and validation error was seen. For batch size 100, the training and validation error increased to 0.178 and 0.177 respectively. A bigger batch size results in faster computation time but the error does not converge compared to using smaller batch sizes which take longer to run. 
\begin{minted}{python}
def q4_1():
    X_train_orig, y_train, X_val_orig, y_val = load_trainval("dynamics.pkl")
    X_train, mu, sigma = standardize_cols(X_train_orig)
    X_val, _, _ = standardize_cols(X_val_orig, mu, sigma)

    """YOUR CODE HERE FOR Q4.1"""
    batch_sizes = [1,10,100]
   
    for batch_size in batch_sizes:
        loss_function = LeastSquaresLoss()
        base_optimizer = GradientDescent()
        learning_rate_getter = ConstantLR(0.0003)
        optimizer = StochasticGradient(base_optimizer, learning_rate_getter, batch_size, max_evals=10)
        model = LinearModel(loss_function, optimizer, check_correctness=False)
        model.fit(X_train, y_train)
        train_err = ((model.predict(X_train) - y_train) ** 2).mean()
        val_err = ((model.predict(X_val) - y_val) ** 2).mean()
        print("Batch size: {:d}\tTraining error: {:.3f}\tValidation error: {:.3f}".format(batch_size, train_err, val_err))
\end{minted}
\end{answer}
\newpage
\subsection{Learning Rates of SGD \pts{6}}

Implement the other unfinished \texttt{LearningRateGetter} classes, which should return the learning rate $\alpha^t$ based on the following
specifications:
\begin{enumerate}
	\item \texttt{ConstantLR}: $\alpha^t = c$.
	\item \texttt{InverseLR}: $\alpha^t = c/t$.
	\item \texttt{InverseSquaredLR}: $\alpha^t = c/t^2$.
	\item \texttt{InverseSqrtLR}: $\alpha^t = c/\sqrt{t}$.
\end{enumerate}
\ask{Submit your code for these three classes.}
\begin{answer}
    \begin{minted}{python}
class ConstantLR(LearningRateGetter):
    def get_learning_rate(self):
        self.num_evals += 1
        return self.multiplier


class InverseLR(LearningRateGetter):
    def get_learning_rate(self):
        """YOUR CODE HERE FOR Q4.2"""
        self.num_evals += 1
        return self.multiplier/self.num_evals


class InverseSquaredLR(LearningRateGetter):
    def get_learning_rate(self):
        """YOUR CODE HERE FOR Q4.2"""
        self.num_evals += 1
        return self.multiplier/(self.num_evals**2)


class InverseSqrtLR(LearningRateGetter):
    def get_learning_rate(self):
        """YOUR CODE HERE FOR Q4.2"""
        self.num_evals += 1
        return self.multiplier/np.sqrt(self.num_evals)
    \end{minted}
\end{answer}
\newpage


\subsection{The Learning Curves (Again) \pts{9}}

Using the four learning rates, produce a plot of learning curves visualizing the behaviour of the objective function $f$ value on the $y$-axis, and the number of stochastic gradient descent epochs (at least 50) on the $x$-axis. Use a batch size of 10. Use $c = 0.1$ for every learning rate function. \blu{Submit this plot and answer the following question. Which step size functions lead to the parameters converging towards a global minimum?}
\begin{answer}
\centerfig{.6}{./figs/learning_curves.png}
The inverse square root step size function leads to a global minimum of around 700 which is the smallest value present in the graph.
\end{answer}
\newpage
\begin{minted}{python}
    @handle("4.3")
def q4_3():
    X_train_orig, y_train, X_val_orig, y_val = load_trainval("dynamics.pkl")
    X_train, mu, sigma = standardize_cols(X_train_orig)
    X_val, _, _ = standardize_cols(X_val_orig, mu, sigma)

    """YOUR CODE HERE FOR Q4.3"""
    c = 0.1
    learning_rate_getters = [
            ConstantLR(c),
            InverseLR(c),
            InverseSquaredLR(c),
            InverseSqrtLR(c)
    ]
    plot_labels = [
        "constant",
        "inverse",
        "inverse_squared",
        "inverse_sqrt"
    ]
    plt.figure()
    for i in range(len(learning_rate_getters)):
        loss_function = LeastSquaresLoss()
        base_optimizer = GradientDescent()
        optimizer = StochasticGradient(base_optimizer, learning_rate_getters[i], 10, max_evals=50)
        model = LinearModel(loss_function, optimizer)
        model.fit(X_train, y_train)
        err_train = np.mean((model.predict(X_train) - y_train) ** 2)
        err_valid = np.mean((model.predict(X_val) - y_val) ** 2)
        plt.plot(model.fs, label=plot_labels[i])
    
    plt.legend()
    plt.xlabel("Epochs")
    plt.ylabel("Objective function f value")
    savefig("learning_curves", plt)


\end{minted}


\clearpage
\section{Very-Short Answer Questions \pts{18}}

\ask{Answer each of the following questions in a sentence or two.}

\begin{enumerate}
\item Assuming we want to use the original features (no change of basis) in a linear model, what is an advantage of the ``other'' normal equations over the original normal equations?

\ans{Using the ``other'' normal equations is an advantage when n < d, i.e. there are more features than examples. This is because we would work on a $n$ x $n$ instead of a $d$ x $d$ instead, resulting in $O(n^2d + n^3)$ in training cost (compared to cubic in $k^3$ in normal equations) and $O(ndt)$ in testing cost.}

\item In class we argued that it's possible to make a kernel version of $k$-means clustering. What would an advantage of kernels be in this context?

\ans{A kernel version of k-means clustering allows for clustering in non-linear space, where clusters are separated by non-linear boundaries. This is because using kernels help capture non-linear relationships between the data and in clusters.}

\item In the language of loss functions and regularization, what is the difference between MLE and MAP?

\ans{MLE is the "likelihood", with a log-likelihood error function, and MAP is the "posterior" which takes the product of the "likelihood" and "prior". The prior introduces in the loss function of MAP a regularization term called the log-prior, which is absent in MLE.}

\item What is the difference between a generative model and a discriminative model?

\ans{A discriminative model only models a relationship between the input data and label. A generative model will learn the input data's distribution and model the relationship between the input data and the label.}

\item In this course, we usually add an offset term to a linear model by transforming to a $Z$ with an added constant feature. How, specifically, can we add an offset term to a kernel-based linear model?

\ans{To add an offset term and ensure the intercept is not zero, we can add a constant value to the inner product result of the kernel, such as the case in this polynomial kernel expression where "1+" is added: $k(x_i,x_j) = (1+x_ix_j^T)^p$}

\item With PCA, is it possible for the loss to increase if $k$ is increased? Briefly justify your answer.

\ans{No. Increasing $k$ increases the number of principal components used, meaning more variance is preserved in the resulting data. Therefore, it would not be possible for loss to increase if $k$ increased.}

\item Why doesn't it make sense to do PCA with $k > d$?

\ans{The purpose of PCA is to reduce dimensions in data. By having more principal components compared to data features, you would be expanding the dimensions rather than reducing them, which negates the efficiency gains from using PCA rather than other models.}

\item In terms of the matrices associated with PCA ($X$, $W$, $Z$, $\hat{X}$), where would a single ``eigenface'' be stored?
\ans{An "eigenface" (eigenvector) is a column in $W$.}

\item What is an advantage and a disadvantage of using stochastic gradient over SVD when doing PCA?

\ans{The advantage  of using stochastic gradient would require less computational resources for large datasets compared to SVD. The disadvantage of using stochastic gradient is that it may take several iterations to converge on the solution, while SVD finds an exact solution.}

\end{enumerate}
\end{document}
